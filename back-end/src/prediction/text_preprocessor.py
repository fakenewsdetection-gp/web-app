import string
import nltk
from nltk.corpus import stopwords
import unicodedata
import inflect
import contractions
import html


nltk.download('stopwords')
stop_words = set(stopwords.words("english"))

nltk.download('punkt')
nltk.download('wordnet')


class TextCleaner:

    def __init__(self):
        self.lem_ = nltk.stem.WordNetLemmatizer()
        self.stem_ = nltk.stem.PorterStemmer()

    @staticmethod
    def fix_contractions(text):
        return contractions.fix(text)

    @staticmethod
    def unescape_html(text):
        return html.unescape(text)

    @staticmethod
    def lemmatize(token, lemmatizer):
        return lemmatizer.lemmatize(token, 'v')

    @staticmethod
    def stem(token, stemmer):
        return stemmer.stem(token)

    @staticmethod
    def remove_non_ascii(word):
        return unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')

    @staticmethod
    def lowercase(word):
        return word.lower()

    @staticmethod
    def remove_punct(word):
        for sym in string.punctuation:
            word = word.replace(sym, '')
        return word

    @staticmethod
    def replace_num(word):
        p = inflect.engine()
        return p.number_to_words(word) if word.isdigit() else word

    @staticmethod
    def filter(tokens, filterset):
        """
        Filters tokens by removing the tokens that appear in filterset.
        """

        return [t for t in tokens if t not in filterset]

    @staticmethod
    def transform(tokens, func):
        """
        Applies a transformation function on a token.
        """

        return [func(t) for t in tokens if func(t).strip()]

    @staticmethod
    def normalize(tokens, remove_nonascii,
                  to_lower, remove_punctuation, replace_num, remove_stopwords):

        if remove_nonascii:
            tokens = TextCleaner.transform(
                tokens, TextCleaner.remove_non_ascii)

        if to_lower:
            tokens = TextCleaner.transform(tokens, TextCleaner.lowercase)

        if remove_punctuation:
            tokens = TextCleaner.transform(tokens, TextCleaner.remove_punct)

        if replace_num:
            tokens = TextCleaner.transform(tokens, TextCleaner.replace_num)

        if remove_stopwords:
            tokens = TextCleaner.filter(tokens, stop_words)

        return tokens

    @staticmethod
    def tokenize(text):
        tokens = [word for sent in nltk.sent_tokenize(
            text) for word in nltk.word_tokenize(sent)]

        return tokens

    def cleanup_text(self, text, lemmatize=True, stem=False, remove_stopwords=True,
                     remove_punctuation=False, remove_nonascii=False,
                     to_lower=True, replace_num=False, fix_contractions=True, unescape_html=True,
                     remove_possessive=False, remove_puncwords=False):

        if unescape_html:
            text = TextCleaner.unescape_html(text)

        if remove_punctuation:
            for ch in string.punctuation:
                text = text.replace(ch, ' ')

        if fix_contractions:
            text = TextCleaner.fix_contractions(text)

        tokens = TextCleaner.tokenize(text)

        if remove_possessive:
            tokens = [token.replace("'s", "").replace(
                "'", "").strip() for token in tokens]

        if remove_puncwords:
            char_tester = str.isalpha if replace_num else str.isalnum
            tokens = [
                token for token in tokens if token and char_tester(token[0])]

        tokens = TextCleaner.normalize(
            tokens, remove_nonascii, to_lower, remove_punctuation, replace_num, remove_stopwords)

        if lemmatize:
            tokens = TextCleaner.transform(
                tokens, lambda token: TextCleaner.lemmatize(token, self.lem_))

        if stem:
            tokens = TextCleaner.transform(
                tokens, lambda token: TextCleaner.stem(token, self.stem_))

        return ' '.join(tokens)
